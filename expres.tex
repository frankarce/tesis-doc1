\chapter{Experiments and Results} \label{expres} 
\section{Experiments}
This study is divided into 2 experiments, the ﬁrst part was conducted with visitors to the museum’s spin Tijuana where 21 users divided into 2 groups participated, the ﬁrst has 10 users between 6 and 11 years 50% men and 50% women and the second group consisted of 11 users 11 and older 66 % men 44 % women. In the second part of the study 41 users of the Technological Institute of Tijuana were used aged between 18 and 50 years 66 % men 34 % women. 
For this study is required to show images, videos and somehow observe the user, that is why many devices that met these needs as a computer to server, 2 laptops, one 7-inch tablet, headphones, 3 projectors, 2 monitors, cameras, sensors (Kinect 2.0) etc. were used. 
In the ﬁrst part of the study users immersed in an exhibitor that gave them visual and aural information as shown in Figure 1 on a topic selected based on various criteria, such as the location of the study on this occasion was a interactive museum and the festival of children’s day approaching we try to use a simple theme and it will bring awareness so we decided to use the theme of water where information was displayed as the uses of the same, its cycle, forms of energy that could generate, health, and the importance of it for the planet. Each of users are generating an account on the system to register their activity, after that we were made a brief explanation of how it worked the exhibitor others based on observation no longer required this explanation. The user took a tablet that was the how the user interacted with the exhibitor, with which had control of the ﬂow of information, as the information was a sequence at the end of the last activity was a questionnaire on the information received besides a survey.
The second part of the study was very similar to the ﬁrst one that had some small modiﬁcations and additions, now the user no longer had control ﬂow only observe and also to observe the user now will take video and sensor, the theme of the exhibition were video games and ﬁlm as the age of the users would go for almost 17 years and older could be topics of their interest. In the same way as in the ﬁrst part each user had an account to record the activity only this occasion the data produced by the sensor and videos captured by the camera would be recorded, and in the same way as in the ﬁrst part at the end we surveyed the users.
For the ﬁrst part of the study we get data from the survey conducted at the end, in the second they are collected and used data from sensors, this data were a bit more complex, ﬁrst we need an observer to evaluate the user manually, where the observer determined whether the user was putting attention to what he saw or distracted and based on that assigned a level of attention on a scale of 1 to 3 where 1 is little attention 2 average attention and 3 very attentive. 
The other way to obtain user information was by Kinect Sensor and is a bit more complex because the sensor provides enough information the sensor detects a user can give: 
• engage 
• looking away 
• happy 
• left eye closed 
• right eye closed
• mouth open 
• mouth moved 
• wearing glasses 
• yaw pitch and roll of the face
Each of these data the sensor assigned one of the following Yes, No, Maybe and Unknown values; unknown data were discarded since in the sensor documentation was saying that this data could be”not sensing” and therefore not considered valid data. Only in the face position was a numerical data, the sensor generated an average of 14 records per second of these values sometimes less for a short time when the sensor lost sight of the user but was usually between 1 to 10 seconds.
Full activity was carried out in 4 minutes 10 seconds so if the sensor not loses sight of the user throughout the activity it generates about 3500 records. From the above list of data we decided to use only engage, looking away happy and others are captured but used as they are not relevant to what we are measuring. To get textual results perform a normalization by a count of each event every 10 seconds since the data generated by the sensor are textual normalized by assigning numbers to strings for example Yes = 2, Maybe = 1 and No = 0 then for every 10 seconds of activity the Yes, No and Maybe was counted.
As the activity lasted 4:10 we decided to count the sensor event every 10 seconds, at the end we gained an average of 140 records for each period of 10 seconds. Here I was a problem, as the number of records varied by user we had to think about how to normalize this data, as records were recorded and distributed yes, maybe, no the solution was to take a percentage of each of the records obtained by which thus we could handle and sensor data.
\section{Results}